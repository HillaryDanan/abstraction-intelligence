# Abstraction-Intelligence

**What makes something intelligent?**

The **Abstraction Primitive Hypothesis (APH)**: intelligence emerges from recursive interaction between symbol formation and compositional structure.

-----

## The Core Claim

**Abstraction is the recursive process of forming and composing symbols.**

```
    [Raw Input] â†’ [Symbol Formation] â†’ [Symbols] â†’ [Composition] â†’ [Composed Structures]
                         â–²                                              â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [Feedback] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Not symbols alone. Not composition alone. Their **mutual refinement through iteration**.

-----

## The Composition Hierarchy

|Type                 |Structure                  |Example                                   |
|---------------------|---------------------------|------------------------------------------|
|**3a: Concatenative**|A + B â†’ AB                 |â€œblue birdâ€                               |
|**3b: Role-filler**  |R(x) + S(y) â†’ R(x)S(y)     |AGENT(dog) + ACTION(chased) + PATIENT(cat)|
|**3c: Recursive**    |A contains [B contains C]  |â€œThe dog [that chased the cat [thatâ€¦]]â€   |
|**3d: Analogical**   |Structure(A) â†’ Structure(B)|atom:nucleus :: solar system:sun          |

**Prediction:** Systems show 3a-3b success with 3c-3d failure. Bees do role-filler (waggle dance), not recursion. LLMs degrade faster on recursive depth than role-filler novelty.

-----

## Embeddedness

Strong interaction requires **novelty detection**â€”recognizing unfamiliar against a background of familiar.

Novelty is relational. An input isnâ€™t novel in itselfâ€”itâ€™s novel *to a subject*. This requires:

1. **Persistent self** â†’ accumulated experience (â€œwhatâ€™s familiar to meâ€)
1. **Self/world distinction** â†’ reference frame for locating novelty
1. **Embeddedness** â†’ what makes persistence and self/world possible

```
Embeddedness â†’ Persistent self â†’ Familiar/unfamiliar distinction â†’ Novelty detection â†’ Pressure to expand primitives â†’ Strong interaction
```

**Five components of embeddedness:**

- Action-consequence contingency
- Feedback closure
- Temporal persistence
- Self-boundary awareness
- Environmental stability

**Operationalizing novelty detection:** To avoid circularity, we specify four measurable criteria independent of embeddedness:

|Criterion                     |Operationalization                                                                       |
|------------------------------|-----------------------------------------------------------------------------------------|
|**Calibration**               |Confidence decreases appropriately for OOD inputs (Guo et al., 2017)                     |
|**Processing differentiation**|Novel inputs trigger distinct internal processing, not just lower confidence             |
|**Response systematicity**    |Novel combinations of familiar components yield appropriate outputs (Lake & Baroni, 2018)|
|**Uncertainty propagation**   |Input novelty propagates to output uncertainty rather than confident confabulation       |

The hypothesis: embeddedness/stakes is necessary for meeting these criteria. Empirically testable.

**Why stakes matter (hypothesis):**

Survival pressure creates asymmetric costsâ€”misclassifying threat as familiar can be fatal; false alarms merely waste energy (Ã–hman et al., 2001). But why *different architecture* rather than just stronger optimization?

- **Phylogenetic vs. ontogenetic:** Stakes during *evolution* select for architectures; stakes during *operation* modulate which capacities are used. The claim: selection under stakes produced architecture capable of construction. LLMs werenâ€™t selected; they were optimized on symmetric loss.
- **Asymmetric vs. symmetric pressure:** Symmetric loss (prediction error) rewards compressionâ€”minimizing average error. Asymmetric loss (survival) rewards worst-case handlingâ€”novel threats must be addressed even at efficiency cost. This selects for construction as a hedge against unbounded novelty. *(Testable: compare architectures trained under symmetric vs. asymmetric loss.)*
- **Candidate architectural feature:** Gating mechanisms that switch between retrieval and construction based on novelty. The locus coeruleus-norepinephrine system modulates exploration vs. exploitation based on uncertainty (Aston-Jones & Cohen, 2005). Biological cognition gates processing mode; LLMs apply identical forward passes regardless of input novelty.
- **On necessity:** Stakes are the known path to construction-capable architecture. Whether alternative paths exist is open. We claim *sufficiency* with confidence; *necessity* remains hypothesis.

**Why 3c-3d specifically (hypothesis):**

- **3a-3b:** Bounded combinatorial space. Can be encountered in training and stored. Pattern matching suffices.
- **3c-3d:** Unbounded generative space. Recursive depth extends indefinitely; analogical mappings span arbitrary domains. Cannot be precomputed.

When target space is unbounded, retrieval failsâ€”*online construction* required. (See [Paper 10](papers/survival_pressure.md).)

**Compression vs. Generation:**

|              |Compression                                    |Generation                                    |
|--------------|-----------------------------------------------|----------------------------------------------|
|**Definition**|Finding structure within training distribution |Constructing outputs with no training coverage|
|**Operation** |Interpolation between known points             |Extrapolation into unbounded space            |
|**3a-3b**     |Sufficient (bounded space is coverable)        |Not required                                  |
|**3c-3d**     |Insufficient (unbounded space exceeds coverage)|Required                                      |

**Prediction:** Compression asymptotes on 3c-3d as coverable space is exhausted. Generation doesnâ€™t asymptote because itâ€™s not coverage-limited. Scaling improves compression; it doesnâ€™t produce generation.

**On LLMs:** Within a conversation, LLMs have weak temporal persistence and action-consequence contingency. But they lack self-boundary awareness, cross-context stability, gating mechanisms, and stakes. This predicts principled 3c-3d limitations that scaling alone cannot overcomeâ€”hypothesis pending empirical test. Whether alternative paths to 3c-3d exist remains open.

-----

## Predictions

|Prediction                                                                                      |Falsification                                                        |
|------------------------------------------------------------------------------------------------|---------------------------------------------------------------------|
|Composition types dissociate (3a-3b vs. 3c-3d)                                                  |No differences found                                                 |
|Recursive depth degrades faster than role-filler novelty                                        |Identical degradation curves                                         |
|Bees: role-filler yes, recursive no                                                             |Bees succeed at recursion                                            |
|Non-embedded systems fail novelty criteria (calibration, systematicity, uncertainty propagation)|System without embeddedness meets all four criteria                  |
|Systematicity failure: novel combinations fail despite component competence                     |Novel combinations succeed proportionally to component familiarity   |
|Systems without stakes plateau on 3c-3d despite scaling                                         |Scaled systems show continued 3c-3d improvement proportional to scale|
|Asymmetric loss training improves 3c-3d over symmetric loss                                     |No difference between loss types                                     |

-----

## Papers

**Core (1â€“10):**

|# |Paper                                                                          |
|--|-------------------------------------------------------------------------------|
|1 |[Abstraction Is All You Need](papers/abstraction_is_all_you_need.md)           |
|2 |[The Computational Structure of Abstraction](papers/abstraction_defined.md)    |
|3 |[Abstraction Beyond Compression](papers/abstraction_beyond_compression.md)     |
|4 |[Abstraction Constrained](papers/abstraction_constrained.md)                   |
|5 |[Prediction Requires Abstraction](papers/prediction_requires_abstraction.md)   |
|6 |[Recursive Abstraction](papers/recursive_abstraction.md)                       |
|7 |[The Developmental Spectrum](papers/abstraction_developmental_spectrum.md)     |
|8 |[Embeddedness and Self/World](papers/embedded_abstraction.md)                  |
|9 |[Self and World](papers/self_world_abstraction.md)                             |
|10|[Survival Pressure and the Origins of Abstraction](papers/survival_pressure.md)|

**Extensions (11â€“17):**

|# |Paper                                                                                |
|--|-------------------------------------------------------------------------------------|
|11|[Consciousness as Emergent Abstraction](papers/consciousness_emergent_abstraction.md)|
|12|[The Hard Problem Reframed](papers/hard_problem_reframed.md)                         |
|13|[Time as Embodied Abstraction](papers/time_embodied_abstraction.md)                  |
|14|[Emotion as Embedded Information](papers/emotion_embedded_information.md)            |
|15|[Social Dynamics](papers/social_dynamics.md)                                         |
|16|[Beyond Large Language Models](papers/beyond_llms.md)                                |
|17|[Dual-Process Theory Reconsidered](papers/dual_process_abstraction.md)               |

-----

## Empirical Research Program

### ğŸ§  Core Framework

[abstraction-intelligence](https://github.com/HillaryDanan/abstraction-intelligence) Â·
[composition-type-dissociation](https://github.com/HillaryDanan/composition-type-dissociation) Â·
[compositional-abstraction](https://github.com/HillaryDanan/compositional-abstraction) Â·
[compositional-dual-process](https://github.com/HillaryDanan/compositional-dual-process) Â·
[embeddedness-calibration](https://github.com/HillaryDanan/embeddedness-calibration) Â·
[emergent-factorization](https://github.com/HillaryDanan/emergent-factorization) Â·
[reasoning-in-vacuum](https://github.com/HillaryDanan/reasoning-in-vacuum)

### ğŸ”„ Self-Reference

[self-referential-dynamics](https://github.com/HillaryDanan/self-referential-dynamics) Â·
[computational-self-construction](https://github.com/HillaryDanan/computational-self-construction) Â·
[ouroboros-learning](https://github.com/HillaryDanan/ouroboros-learning) Â·
[recursive-reality](https://github.com/HillaryDanan/recursive-reality)

### â±ï¸ Temporal

[TIDE](https://github.com/HillaryDanan/TIDE) Â·
[TIDE-resonance](https://github.com/HillaryDanan/TIDE-resonance) Â·
[TIDE-analysis](https://github.com/HillaryDanan/TIDE-analysis) Â·
[temporal-coherence-llm](https://github.com/HillaryDanan/temporal-coherence-llm) Â·
[temporal-myopia-llm](https://github.com/HillaryDanan/temporal-myopia-llm) Â·
[llm-time-decay](https://github.com/HillaryDanan/llm-time-decay) Â·
[curved-cognition](https://github.com/HillaryDanan/curved-cognition)

### ğŸŒ Embodiment

[embodied-cognition](https://github.com/HillaryDanan/embodied-cognition) Â·
[physical-grounding-llm](https://github.com/HillaryDanan/physical-grounding-llm) Â·
[TERRA-embodied-interpretability](https://github.com/HillaryDanan/TERRA-embodied-interpretability)

### ğŸª Consciousness

[BIND](https://github.com/HillaryDanan/BIND) Â·
[comparative-consciousness-llms](https://github.com/HillaryDanan/comparative-consciousness-llms) Â·
[hexagonal-consciousness-suite](https://github.com/HillaryDanan/hexagonal-consciousness-suite) Â·
[computational-emergence-theory](https://github.com/HillaryDanan/computational-emergence-theory)

### ğŸ‘¥ Social

[reciprocal-mirroring-emergence](https://github.com/HillaryDanan/reciprocal-mirroring-emergence) Â·
[game-theory-trust-suite](https://github.com/HillaryDanan/game-theory-trust-suite) Â·
[trust-calibration-framework](https://github.com/HillaryDanan/trust-calibration-framework)

### ğŸ—£ï¸ Language

[linguistic-dynamics-theory](https://github.com/HillaryDanan/linguistic-dynamics-theory) Â·
[linguistic-memory-framework](https://github.com/HillaryDanan/linguistic-memory-framework) Â·
[cross-linguistic-attention-dynamics](https://github.com/HillaryDanan/cross-linguistic-attention-dynamics) Â·
[benign-violations](https://github.com/HillaryDanan/benign-violations)

### ğŸ”¬ Geometry

[causal-attention-geometry](https://github.com/HillaryDanan/causal-attention-geometry) Â·
[multi-geometric-attention](https://github.com/HillaryDanan/multi-geometric-attention) Â·
[relativistic-interpretability](https://github.com/HillaryDanan/relativistic-interpretability) Â·
[spectral-representations](https://github.com/HillaryDanan/spectral-representations)

### ğŸ§ª LLM Testing

[llm-habituation-patterns](https://github.com/HillaryDanan/llm-habituation-patterns) Â·
[nonlinear-dialogue-dynamics](https://github.com/HillaryDanan/nonlinear-dialogue-dynamics) Â·
[paradox-induced-oscillations](https://github.com/HillaryDanan/paradox-induced-oscillations) Â·
[retroactive-causality](https://github.com/HillaryDanan/retroactive-causality) Â·
[claude-emergence-patterns](https://github.com/HillaryDanan/claude-emergence-patterns)

### ğŸ”§ Architecture

[information-atoms](https://github.com/HillaryDanan/information-atoms) Â·
[hexagonal-vision-research](https://github.com/HillaryDanan/hexagonal-vision-research) Â·
[computational-substrates](https://github.com/HillaryDanan/computational-substrates) Â·
[cognitive-architectures-ai](https://github.com/HillaryDanan/cognitive-architectures-ai)

### ğŸ“Š Tools

[pattern-analyzer](https://github.com/HillaryDanan/pattern-analyzer) Â·
[concrete-overflow-detector](https://github.com/HillaryDanan/concrete-overflow-detector)

-----

## References

Aston-Jones, G., & Cohen, J. D. (2005). An integrative theory of locus coeruleus-norepinephrine function: Adaptive gain and optimal performance. *Annual Review of Neuroscience*, 28, 403-450.

Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and cognitive architecture: A critical analysis. *Cognition*, 28(1-2), 3-71.

Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). On calibration of modern neural networks. *ICML*.

Lake, B., & Baroni, M. (2018). Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. *ICML*.

Ã–hman, A., Flykt, A., & Esteves, F. (2001). Emotion drives attention: Detecting the snake in the grass. *Journal of Experimental Psychology: General*, 130(3), 466-478.

-----

## Author

**Hillary Danan, PhD** Â· Cognitive Neuroscience

-----

*â€œAbstraction is all you need ;)â€*
