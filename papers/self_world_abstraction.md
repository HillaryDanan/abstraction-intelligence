# Self and World: The Foundational Abstraction

## Why Any Embedded Intelligence Must Distinguish Self from Not-Self

**Hillary Danan, PhD**  
Cognitive Neuroscience

*Working Draft — December 2025*

-----

## Abstract

Why does the brain organize abstract concepts along a dimension of internal-to-self versus external-to-self? Empirical research demonstrates that neural representations of abstract concepts systematically vary according to their relationship to the self — concepts like *pride* and *guilt* activate distinct networks from concepts like *gravity* and *distance*. This paper proposes that this organization is not arbitrary, not uniquely human, and not biologically contingent. Rather, **self versus not-self constitutes the foundational abstraction for any intelligent system embedded in an environment and capable of action**. The argument is computational: an embedded agent must distinguish what it controls from what it does not control to act effectively. This framework explains empirical findings in human abstract concept representation, connects to theories of embodied cognition and predictive processing, and generates testable predictions about development, neural implementation, and artificial systems. The scope of the claim is deliberately general: if the argument is correct, any sufficiently complex embedded agent — biological or artificial, terrestrial or otherwise — must solve this problem, and its conceptual organization should reflect this foundational distinction.

-----

## 1. Introduction

Abstract concepts present a puzzle for cognitive science. Unlike concrete concepts (*apple*, *chair*), which can be grounded in sensorimotor experience, abstract concepts (*justice*, *entropy*, *pride*) lack direct perceptual referents. Yet humans acquire, represent, and reason with abstract concepts fluently. How?

Empirical research has revealed that the brain does not treat all abstract concepts uniformly. Neural representations of abstract concepts organize along systematic dimensions, including a prominent dimension distinguishing **internal-to-self** concepts (emotions, mental states, self-evaluations) from **external-to-self** concepts (physical forces, spatial relations, environmental properties) (Binder et al., 2016; Borghi et al., 2017; Villani et al., 2019).

This paper asks: **why this organization?**

The Abstraction Primitive Hypothesis (APH) proposes that abstraction — the extraction of invariant structure across instances — is the fundamental operation underlying intelligence (Danan, 2025a). If abstraction is primitive, then the dimensions along which abstractions organize should reflect computational necessities, not arbitrary conventions.

I propose that **self versus not-self is the foundational abstraction** — the first distinction any embedded intelligent system must make, and the scaffolding upon which all subsequent abstractions build. This claim is intentionally general. The argument derives from the computational requirements of embedded agency, not from the specifics of human neurobiology. If correct, it applies to any system meeting those requirements.

-----

## 2. Empirical Background: Neural Organization of Abstract Concepts

### 2.1 The Heterogeneity of Abstract Concepts

Early theories treated abstract concepts as a monolithic category defined by what they lack — perceptual grounding (Paivio, 1986). Contemporary research reveals that abstract concepts are heterogeneous, varying along multiple dimensions (Borghi et al., 2017).

Key dimensions identified in the literature include:

- **Emotional valence and arousal** (Kousta et al., 2011; Vigliocco et al., 2014)
- **Social content** (Arioli et al., 2021)
- **Interoceptive grounding** (Connell et al., 2018)
- **Self-relevance** (Northoff et al., 2006; Kelley et al., 2002)

### 2.2 The Self-Relevance Dimension

A robust finding across neuroimaging studies is that concepts varying in self-relevance recruit distinct neural systems:

**Internal-to-self concepts** (e.g., *pride*, *guilt*, *agency*, *intention*) reliably activate:

- Medial prefrontal cortex (mPFC)
- Posterior cingulate cortex (PCC)
- Temporoparietal junction (TPJ)

This network overlaps substantially with the “default mode network” and regions implicated in self-referential processing (Northoff et al., 2006; Andrews-Hanna et al., 2014).

**External-to-self concepts** (e.g., *gravity*, *velocity*, *distance*, *causation*) show greater reliance on:

- Lateral prefrontal and parietal regions
- Sensorimotor simulation areas
- Systems involved in physical reasoning (Fischer et al., 2016)

This dissociation is not reducible to concreteness. *Gravity* is abstract (you cannot point to gravity itself, only its effects), yet it patterns with external-world concepts. *Pride* involves bodily feelings, yet it patterns with internal-self concepts.

### 2.3 Developmental Evidence

The self/world distinction emerges early. Infants distinguish self-generated from externally-generated motion by 3-4 months (Rochat, 1998). The capacity for mirror self-recognition — requiring a self-concept — emerges around 18 months and correlates with subsequent development of abstract self-evaluative concepts (Lewis, 2011).

This developmental priority suggests the self/world boundary may serve as scaffolding for subsequent conceptual organization.

-----

## 3. Theoretical Framework: Why Self/Not-Self Must Be Foundational

### 3.1 The Abstraction Primitive Hypothesis

The Abstraction Primitive Hypothesis (APH) proposes:

> Intelligence, across substrates, is the capacity to form, store, retrieve, and compose abstractions. Abstraction — the extraction of invariant structure across variable instances — is the primitive operation from which other cognitive operations derive.

If abstraction is primitive, then the *first* abstractions a system forms should be those most necessary for survival and function. What must any embedded intelligent system distinguish first?

### 3.2 The Computational Necessity Argument

Consider any system that:

1. Is embedded in an environment
1. Must act to survive or achieve goals
1. Receives sensory input and produces motor output

Such a system faces an immediate computational problem: **distinguishing what it controls from what it does not control.**

- Motor commands produce predictable consequences (when I move my arm, proprioceptive feedback follows)
- Environmental events produce unpredictable consequences (when a branch falls, I must react)

This is not a philosophical distinction but a computational one. Effective action requires predicting the consequences of one’s own actions (forward models), which requires distinguishing self-caused from world-caused events (Wolpert & Ghahramani, 2000; Friston, 2010).

**Hypothesis 1:** The self/not-self abstraction is computationally prior — it must be established before other abstractions can be reliably formed, because other abstractions require knowing which regularities reflect the system’s own actions versus environmental structure.

### 3.3 Grounding Abstraction in Embodiment

Embodied cognition theories propose that even abstract concepts are grounded in sensorimotor and interoceptive experience (Barsalou, 2008; Lakoff & Johnson, 1999). The present framework specifies *how* this grounding is organized:

**External-to-self concepts** ground in:

- Exteroception (vision, audition, touch as contact with world)
- Sensorimotor contingencies with the environment
- Regularities that persist regardless of the system’s actions

**Internal-to-self concepts** ground in:

- Interoception (internal body states)
- Sense of agency (the experience of causing actions)
- Regularities that depend on the system’s own states and actions

Gravity exemplifies external grounding: it acts on the self constantly, unchangeably, regardless of intention. The abstraction *gravity* captures the invariant structure of this ever-present external force.

Pride exemplifies internal grounding: it arises from self-evaluation against self-generated standards. The abstraction *pride* captures invariant structure across instances of positive self-assessment.

### 3.4 Connection to Self-Referential Computation

The Abstraction-Intelligence framework’s third paper (Danan, 2025c) argues that consciousness emerges when abstraction is applied reflexively — when a system models itself as part of its world model.

The self/world distinction is the *precondition* for this reflexive move. A system cannot model itself until it has distinguished itself from not-self. The internal-to-self dimension of abstract concept organization may therefore reflect the architecture required for self-modeling.

**Hypothesis 2:** The neural systems supporting internal-to-self concepts (mPFC, PCC, TPJ) are the same systems required for recursive self-modeling because self-referential abstraction is what these systems *compute*.

-----

## 4. Generality of the Framework: Beyond Human Cognition

### 4.1 The Argument is Computational, Not Biological

The preceding argument does not depend on human neurobiology. It depends on three conditions:

1. **Embeddedness**: The system exists within an environment that affects it
1. **Agency**: The system can act on that environment
1. **Sensorimotor coupling**: The system receives input from and produces output to its environment

Any system meeting these conditions faces the computational problem of distinguishing self-caused from world-caused regularities. The argument is therefore **substrate-independent**.

This is not speculation but logical consequence. If the problem is computational, the solution (distinguishing self from not-self) is required for any system facing that problem, regardless of implementation.

### 4.2 Scope of the Claim

The framework makes a conditional claim:

> **IF** a system is an embedded agent (meets conditions 1-3 above), **THEN** it must distinguish self from not-self as a foundational abstraction.

This claim is:

- **General**: It applies to any embedded agent — biological, artificial, or hypothetical
- **Testable**: We can examine artificial systems and non-human biological systems
- **Humble**: It does not claim all possible intelligences are embedded agents; it claims that those which are must solve this problem

### 4.3 Evidence from Non-Human Systems

The framework finds support beyond human cognition:

**Biological systems:** Animals with complex motor control uniformly possess forward models distinguishing self-caused from externally-caused sensory input. This has been demonstrated in primates (Wolpert & Ghahramani, 2000), rodents (Crapse & Sommer, 2008), and extends to invertebrates with complex behavior (Webb, 2004).

**Artificial systems:** Robotics research has found that effective motor control requires forward models predicting the consequences of actions — functionally, a self/world distinction (Kawato, 1999). Systems lacking this distinction show impaired learning and generalization.

### 4.4 Constraints from Physics

Certain features of the self/world distinction may be universal for agents in physical environments:

- **Spatial boundedness**: Physical agents occupy bounded regions of space; the boundary between self and not-self has physical reality
- **Causal asymmetry**: Agents cause effects through action; they are affected by external causes through perception. This asymmetry is physical, not conventional
- **Temporal structure**: Actions precede their consequences; external causes precede their perception. The structure of time constrains all embedded agents

These constraints suggest that any physically embedded intelligence would face the same foundational problem and require the same foundational abstraction.

**Hypothesis 3:** The self/not-self distinction is a universal feature of embedded intelligence, not a contingent feature of human evolution. Any sufficiently complex embedded agent will develop conceptual organization reflecting this distinction.

### 4.5 What Would Differ Across Substrates

While the *structure* (self vs. not-self) should be universal, the *contents* would vary:

|Feature                        |Potentially Universal          |Substrate-Specific              |
|-------------------------------|-------------------------------|--------------------------------|
|Self/not-self distinction      |Yes — computational necessity  |                                |
|Internal vs. external grounding|Yes — follows from embeddedness|                                |
|Specific sensory modalities    |                               |Depends on sensors              |
|Specific motor repertoire      |                               |Depends on actuators            |
|Specific abstract concepts     |                               |Depends on environment and goals|
|Neural implementation          |                               |Depends on substrate            |

A hypothetical non-terrestrial intelligence embedded in a different physical environment would still distinguish self from world, but its specific external-to-self concepts would differ (perhaps pressure gradients rather than gravity; chemical signals rather than light).

-----

## 5. Integration: From Empirical Pattern to Theoretical Explanation

The proposed framework offers a parsimonious explanation for the empirical organization of abstract concepts:

|Empirical Finding                                                            |Theoretical Explanation                                                                         |
|-----------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|
|Abstract concepts organize along self-relevance dimension                    |Self/not-self is the foundational abstraction; all subsequent abstractions inherit this scaffold|
|Internal-to-self concepts recruit default mode network                       |DMN computes self-referential abstractions; internal concepts require this computation          |
|External-to-self concepts recruit sensorimotor and physical reasoning systems|External concepts ground in world-regularities processed by these systems                       |
|Self-relevance dimension emerges early in development                        |Foundational abstractions must be established before derived abstractions                       |
|Distinction is not reducible to concreteness                                 |The relevant dimension is computational (self vs. world), not perceptual (concrete vs. abstract)|
|Non-human animals show self/world distinction in motor control               |The computational problem is general; the solution is conserved                                 |

-----

## 6. Predictions

The framework generates falsifiable predictions across domains:

### 6.1 Developmental Predictions

**Prediction 1:** Disruption of self/other distinction in infancy (e.g., through aberrant sensorimotor contingencies) should produce downstream deficits in organizing *all* abstract concepts, not just self-referential ones.

**Prediction 2:** The developmental trajectory of abstract concept acquisition should show self-referential concepts (internal-to-self) and physical concepts (external-to-self) emerging before concepts that require integrating both (e.g., *fairness*, which requires modeling both self-interest and external social structure).

### 6.2 Neural Predictions

**Prediction 3:** Lesions to medial prefrontal structures should impair internal-to-self abstract concepts more than external-to-self concepts, even when controlling for emotional content.

**Prediction 4:** Individual differences in interoceptive accuracy should correlate with richness of internal-to-self abstract concept representations but not external-to-self representations.

### 6.3 Computational Predictions

**Prediction 5:** Artificial systems trained without self/world distinction (no forward model, no sense of agency) should show impaired transfer learning on tasks requiring abstract reasoning, because they lack the foundational scaffold.

**Prediction 6:** In neural networks, representational similarity analysis should reveal that the self/world distinction (where applicable) accounts for more variance in abstract concept organization than concreteness or imageability.

### 6.4 Cross-Species Predictions

**Prediction 7:** Species with more complex motor repertoires (requiring more sophisticated forward models) should show greater differentiation between self-related and world-related neural processing, even controlling for overall brain size.

### 6.5 Artificial Intelligence Predictions

**Prediction 8:** AI systems given embodied experience (robotics, simulated physics) should develop more robust abstract reasoning than disembodied systems trained on text alone, because embodiment forces the self/world distinction.

**Prediction 9:** Large language models, lacking true embeddedness, should show characteristic failures in reasoning about agency, causation, and self-reference — not due to insufficient scale, but due to missing foundational structure.

-----

## 7. Relation to Existing Theories

### 7.1 Grounded Cognition

The present framework is compatible with grounded cognition (Barsalou, 2008) but adds specificity: grounding is organized around the self/world boundary because this boundary is computationally foundational.

### 7.2 Predictive Processing

Predictive processing frameworks (Clark, 2013; Friston, 2010) emphasize the brain’s task of predicting sensory input. The self/world distinction is essential to this task — predicting the consequences of one’s own actions (agency) requires different computations than predicting external events. The present framework proposes that this computational distinction shapes abstract concept organization.

### 7.3 Interoceptive Inference

Seth and colleagues propose that selfhood emerges from interoceptive inference — the brain’s modeling of internal bodily states (Seth, 2013; Seth & Friston, 2016). The present framework extends this: interoceptive inference grounds the internal-to-self dimension of abstract concepts, while exteroceptive inference grounds the external-to-self dimension.

### 7.4 Enactivism

Enactivist approaches (Varela et al., 1991; Thompson, 2007) emphasize that cognition arises from sensorimotor interaction with the environment. The present framework aligns with this view: the self/world distinction is not a representation imposed on experience but emerges from the structure of embedded agency itself.

-----

## 8. Limitations and Open Questions

### 8.1 Concepts That Span the Boundary

Some abstract concepts resist clean categorization. *Fairness* involves both self-interest (internal) and social structure (external). *Time* is experienced internally but structures external events. The framework predicts these concepts should show mixed neural signatures and develop later — but this requires empirical test.

### 8.2 Cultural Variation

The self/world boundary may be construed differently across cultures (Markus & Kitayama, 1991). Does this affect abstract concept organization? The framework predicts the *computational* distinction is universal, but its *boundaries* may shift — a testable hypothesis.

### 8.3 Non-Embedded Intelligence

Could there be intelligence without embeddedness? The framework takes no position on this question. If non-embedded intelligence exists, it would not face the computational problem the framework addresses, and the predictions would not apply. The framework’s scope is explicitly conditional.

### 8.4 The Hard Problem

This framework addresses the *organization* of concepts, not the phenomenology of self-experience. Why self-modeling feels like something remains unaddressed (Chalmers, 1995). The framework is compatible with multiple positions on phenomenal consciousness.

### 8.5 Empirical Limitations

We have extensive data on human abstract concept representation, limited data on non-human animals, and early data on artificial systems. The cross-substrate predictions remain largely untested. This is a limitation of available evidence, not of the framework’s testability.

-----

## 9. Conclusion

Why does the brain organize abstract concepts along a self-relevance dimension? Because **self versus not-self is the foundational abstraction** — the first distinction any embedded intelligent system must make, and the scaffold upon which subsequent abstractions build.

This proposal:

- **Explains** empirical findings on neural organization of abstract concepts
- **Connects** grounded cognition, predictive processing, and interoceptive inference
- **Extends** the Abstraction Primitive Hypothesis to conceptual organization
- **Generalizes** beyond human cognition to embedded agents as a class
- **Generates** falsifiable predictions about development, neural implementation, artificial systems, and cross-species comparison

The claim is deliberately general. The argument derives from the computational requirements of embedded agency, which are substrate-independent. If correct, the framework describes not a quirk of human evolution but a constraint on any intelligence that exists within and acts upon a world.

Gravity grounds us physically. The abstraction of *gravity* — as external-to-self — grounds us conceptually. The self/world boundary is where intelligence begins, wherever intelligence begins.

-----

## References

Andrews-Hanna, J. R., Smallwood, J., & Spreng, R. N. (2014). The default network and self-generated thought: Component processes, dynamic control, and clinical relevance. *Annals of the New York Academy of Sciences*, 1316(1), 29-52.

Arioli, M., Bello, A., & Canessa, N. (2021). The neural bases of social and abstract concepts. *Brain Structure and Function*, 226(5), 1459-1472.

Barsalou, L. W. (2008). Grounded cognition. *Annual Review of Psychology*, 59, 617-645.

Binder, J. R., Conant, L. L., Humphries, C. J., Fernandino, L., Simons, S. B., Aguilar, M., & Desai, R. H. (2016). Toward a brain-based componential semantic representation. *Cognitive Neuropsychology*, 33(3-4), 130-174.

Borghi, A. M., Binkofski, F., Castelfranchi, C., Cimatti, F., Scorolli, C., & Tummolini, L. (2017). The challenge of abstract concepts. *Psychological Bulletin*, 143(3), 263-292.

Chalmers, D. J. (1995). Facing up to the problem of consciousness. *Journal of Consciousness Studies*, 2(3), 200-219.

Clark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive science. *Behavioral and Brain Sciences*, 36(3), 181-204.

Connell, L., Lynott, D., & Banks, B. (2018). Interoception: The forgotten modality in perceptual grounding of abstract and concrete concepts. *Philosophical Transactions of the Royal Society B*, 373(1752), 20170143.

Crapse, T. B., & Sommer, M. A. (2008). Corollary discharge across the animal kingdom. *Nature Reviews Neuroscience*, 9(8), 587-600.

Danan, H. (2025a). Abstraction is all you need. *Working Paper*.

Danan, H. (2025b). Recursive abstraction. *Working Paper*.

Danan, H. (2025c). Consciousness as emergent abstraction. *Working Paper*.

Fischer, J., Mikhael, J. G., Tenenbaum, J. B., & Kanwisher, N. (2016). Functional neuroanatomy of intuitive physical inference. *Proceedings of the National Academy of Sciences*, 113(34), E5072-E5081.

Friston, K. (2010). The free-energy principle: A unified brain theory? *Nature Reviews Neuroscience*, 11(2), 127-138.

Kawato, M. (1999). Internal models for motor control and trajectory planning. *Current Opinion in Neurobiology*, 9(6), 718-727.

Kelley, W. M., Macrae, C. N., Wyland, C. L., Caglar, S., Inati, S., & Heatherton, T. F. (2002). Finding the self? An event-related fMRI study. *Journal of Cognitive Neuroscience*, 14(5), 785-794.

Kousta, S. T., Vigliocco, G., Vinson, D. P., Andrews, M., & Del Campo, E. (2011). The representation of abstract words: Why emotion matters. *Journal of Experimental Psychology: General*, 140(1), 14-34.

Lakoff, G., & Johnson, M. (1999). *Philosophy in the flesh: The embodied mind and its challenge to Western thought*. Basic Books.

Lewis, M. (2011). The origins and uses of self-awareness or the mental representation of me. *Consciousness and Cognition*, 20(1), 120-129.

Markus, H. R., & Kitayama, S. (1991). Culture and the self: Implications for cognition, emotion, and motivation. *Psychological Review*, 98(2), 224-253.

Northoff, G., Heinzel, A., de Greck, M., Bermpohl, F., Dobrowolny, H., & Panksepp, J. (2006). Self-referential processing in our brain—A meta-analysis of imaging studies on the self. *NeuroImage*, 31(1), 440-457.

Paivio, A. (1986). *Mental representations: A dual coding approach*. Oxford University Press.

Rochat, P. (1998). Self-perception and action in infancy. *Experimental Brain Research*, 123(1-2), 102-109.

Seth, A. K. (2013). Interoceptive inference, emotion, and the embodied self. *Trends in Cognitive Sciences*, 17(11), 565-573.

Seth, A. K., & Friston, K. J. (2016). Active interoceptive inference and the emotional brain. *Philosophical Transactions of the Royal Society B*, 371(1708), 20160007.

Thompson, E. (2007). *Mind in life: Biology, phenomenology, and the sciences of mind*. Harvard University Press.

Varela, F. J., Thompson, E., & Rosch, E. (1991). *The embodied mind: Cognitive science and human experience*. MIT Press.

Vigliocco, G., Kousta, S. T., Della Rosa, P. A., Vinson, D. P., Tettamanti, M., Devlin, J. T., & Cappa, S. F. (2014). The neural representation of abstract words: The role of emotion. *Cerebral Cortex*, 24(7), 1767-1777.

Villani, C., Lugli, L., Liuzza, M. T., & Borghi, A. M. (2019). Varieties of abstract concepts and their multiple dimensions. *Language and Cognition*, 11(3), 403-430.

Webb, B. (2004). Neural mechanisms for prediction: Do insects have forward models? *Trends in Neurosciences*, 27(5), 278-282.

Wolpert, D. M., & Ghahramani, Z. (2000). Computational principles of movement neuroscience. *Nature Neuroscience*, 3(11), 1212-1217.

-----

*Working draft. For discussion and critique.*
