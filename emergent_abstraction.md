# Consciousness as Emergent Abstraction: A Computational Necessity Framework

**Hillary Danan**  
*December 2025*

---

## Abstract

We propose that consciousness is not an epiphenomenal mystery but a computationally necessary abstraction layer that emerges when biological systems reach sufficient complexity to require integrated self-monitoring for adaptive behavior. Drawing on global workspace theory, predictive processing frameworks, computational complexity theory, and neural dynamics, we argue that consciousness serves a specific functional role: monitoring and integrating self-referential transformations in complex adaptive systems. This framework—"Abstraction as Necessity" (AaN)—generates testable predictions, offers a parsimonious account of why consciousness feels unified despite distributed neural processing, and suggests principled criteria for evaluating consciousness in biological and artificial systems.

---

## 1. Introduction

The "hard problem" of consciousness—why subjective experience exists at all—has resisted solution for decades (Chalmers, 1995). We suggest this framing may itself be the obstacle. Rather than asking why consciousness exists as a metaphysical puzzle, we ask: under what computational conditions would consciousness become *necessary*?

This reframing shifts the question from philosophy to engineering: What kind of system would require an integrated monitoring layer? When does complexity demand abstraction?

### 1.1 The Core Thesis

Consciousness emerges as a functional abstraction layer when a system meets three conditions:

1. **Sufficient complexity**: The system contains enough interdependent processes that direct monitoring of all components becomes computationally intractable
2. **Self-transformation**: The system is continuously changing, and its rate of change depends on its current state
3. **Adaptive pressure**: The system must respond appropriately to maintain viability (homeostasis, survival, reproduction)

Under these conditions, we hypothesize that an abstraction layer—a simplified, integrated model of the system's own state—becomes computationally necessary. This layer is consciousness.

### 1.2 Theoretical Context: Abstraction as Primitive

This paper extends a broader theoretical framework—the Abstraction Primitive Hypothesis (Danan, 2025)—proposing that abstraction is the fundamental operation underlying intelligence. Abstraction, formally defined as a mapping that preserves task-relevant structure while discarding task-irrelevant detail, appears across cognitive domains:

- **Perception**: Abstracting sensory input into objects and categories
- **Memory**: Abstracting experiences into retrievable schemas
- **Language**: Abstracting concepts into transmissible symbols
- **Reasoning**: Abstracting patterns into manipulable rules

Consciousness, in this view, is what abstraction looks like when the system being abstracted is oneself. The same operation that compresses external complexity into manageable representations must, at sufficient system complexity, be applied reflexively—yielding an integrated self-model that we experience as consciousness.

This suggests a unified account of mind: cognition is abstraction applied outward; consciousness is abstraction applied inward.

---

## 2. Current Landscape: Major Theories of Consciousness

### 2.1 Global Workspace Theory (GWT)

Baars (1988) proposed that consciousness arises from a "global workspace"—a cognitive architecture where specialized modules compete for access to a shared broadcast system. Information that wins this competition becomes globally available and thus conscious.

Empirical support for GWT comes from neuroimaging studies demonstrating widespread cortical activation during conscious perception versus unconscious processing (Dehaene et al., 2006; Dehaene & Changeux, 2011).

*Relation to current framework*: GWT describes the mechanism of conscious access but does not fully address why such a workspace would evolve. Our framework suggests the workspace exists because integrated monitoring became computationally necessary at a certain threshold of complexity.

### 2.2 Integrated Information Theory (IIT)

Tononi (2004, 2008) proposes that consciousness corresponds to integrated information (Φ)—the amount of information generated by a system above and beyond its parts.

IIT makes specific predictions about the neural correlates of consciousness and has been tested using perturbational complexity index (PCI) measures (Casali et al., 2013).

*Relation to current framework*: IIT's emphasis on integration aligns with our proposal—integration is precisely what an abstraction layer provides. However, IIT treats Φ as constitutive of consciousness (consciousness *is* integrated information), whereas we treat integration as functional (consciousness *does* integration because it's computationally required).

### 2.3 Higher-Order Theories (HOT)

Higher-order theories propose that a mental state is conscious when it is the object of a higher-order representation—roughly, when you have a thought about the thought (Rosenthal, 2005; Lau & Rosenthal, 2011).

*Relation to current framework*: HOT captures something essential—the self-referential nature of consciousness. Our framework grounds this self-reference in computational necessity: you need a model of your own states to monitor complex self-transformation.

### 2.4 Predictive Processing / Active Inference

The predictive processing framework (Friston, 2010; Clark, 2013) proposes that brains are prediction machines, constantly generating models of expected input and updating based on prediction error.

*Relation to current framework*: Predictive processing provides the mechanism by which abstraction operates. The brain's generative models are abstractions—compressed representations that predict (and thus monitor) ongoing states. Our framework adds that at sufficient complexity, these models must include a model of the self as a changing system, yielding consciousness.

### 2.5 Gap in Current Theories

Current theories describe mechanisms and correlates but generally do not address the computational necessity question: Why would evolution produce consciousness rather than zombie-equivalent processing? Our framework offers an answer: at sufficient complexity, integrated self-monitoring isn't optional—it's the only computationally tractable solution.

---

## 3. Computational Complexity and the Necessity of Abstraction

### 3.1 The Curse of Dimensionality

In complex systems, the number of possible states grows exponentially with the number of components. A system with *n* binary components has 2^n possible states. Direct monitoring of all states rapidly becomes intractable (Bellman, 1961).

### 3.2 Abstraction as Compression

Abstraction reduces dimensionality by identifying relevant features and discarding irrelevant detail. This is formalized in rate-distortion theory (Shannon, 1948), the information bottleneck method (Tishby et al., 2000), and deep learning feature hierarchies (Bengio et al., 2013).

### 3.3 When Must a System Model Itself?

A system must model itself when:

1. Its continued viability depends on appropriate responses to environmental challenges
2. Appropriate responses depend on current internal state
3. Internal state is too complex to monitor directly

Under these conditions, a compressed self-model—an abstraction of one's own state—becomes necessary. This self-model is the minimal necessary structure for adaptive behavior in complex organisms.

### 3.4 Toward Specifying the Complexity Threshold

A key open question is formalizing the complexity threshold at which consciousness becomes necessary. We do not yet have a precise specification, but candidate metrics include:

- **Neuron count and connectivity density**: The human brain contains approximately 86 billion neurons with 10^14–10^15 synaptic connections (Azevedo et al., 2009). However, raw count is likely insufficient—network topology matters.
- **Effective connectivity and integration**: Measures like Tononi's Φ attempt to capture integrated information. High Φ may index systems requiring abstraction for self-monitoring.
- **Hierarchical depth**: The number of processing levels between sensory input and motor output. Deeper hierarchies may require higher-order monitoring.
- **Recurrent connectivity**: Systems with extensive recurrence must track their own dynamics, potentially necessitating self-models.

The threshold is likely a combination of these factors rather than any single metric. Formalizing this remains a priority for future work.

### 3.5 The Binding Problem as Abstraction Problem

The "binding problem"—how distributed neural processes produce unified experience—has long puzzled consciousness researchers (Treisman, 1996).

Our framework dissolves this problem: unified experience *is* the abstraction. The abstraction layer doesn't need to "bind" distributed processes—it models them at a level where binding is the default representation. Just as a manager doesn't need to "bind" the activities of employees—they simply have a job-level model, not a muscle-movement-level model—consciousness represents integrated states because that's the level of abstraction at which monitoring is tractable.

---

## 4. Neural Implementation: Where Abstraction Meets Anatomy

### 4.1 Hierarchical Processing

Cortical processing is hierarchical, with successive layers extracting increasingly abstract features (Felleman & Van Essen, 1991; Markov et al., 2014).

We propose that consciousness corresponds to a specific level in this hierarchy—the level at which self-state modeling becomes computationally tractable. This is likely not a single brain region but a distributed process at a particular level of abstraction.

### 4.2 Default Mode Network and Self-Reference

The default mode network (DMN)—including medial prefrontal cortex, posterior cingulate, and angular gyrus—activates during self-referential processing (Raichle et al., 2001; Buckner et al., 2008).

The DMN may implement aspects of the self-monitoring abstraction layer. Its deactivation during externally-focused tasks may reflect shifting of monitoring resources, not absence of consciousness.

### 4.3 Predictive Coding Architecture

Predictive coding models propose that cortical hierarchies implement prediction and prediction-error signaling, with higher levels providing predictions and lower levels providing error signals (Rao & Ballard, 1999; Friston, 2005).

This architecture naturally supports abstraction: higher levels encode compressed models, lower levels encode residuals (deviations from predictions). Consciousness may correspond to the highest level(s) of this hierarchy that include self-modeling.

---

## 5. Testable Predictions

A theory's value lies partly in its falsifiability. The Abstraction as Necessity (AaN) framework makes the following testable claims:

### 5.1 Complexity Threshold Prediction

There exists a threshold of system complexity below which integrated self-monitoring is unnecessary and above which it becomes computationally necessary. Organisms below this threshold should show adaptive behavior without evidence of unified consciousness; organisms above should show evidence of integrated self-modeling.

*Potential test*: Comparative studies across species examining the relationship between neural complexity metrics (neuron count, connectivity patterns, hierarchical depth) and behavioral/neural signatures of integrated processing. Species with similar neuron counts but different connectivity architectures may show divergent signatures of integration.

### 5.2 Abstraction Level Prediction

Disruptions to higher levels of neural hierarchy should disrupt conscious experience while leaving lower-level processing intact. Disruptions to lower levels should alter the content of consciousness without eliminating consciousness itself.

*Potential test*: Lesion studies, TMS studies, and pharmacological interventions targeting different hierarchical levels, correlated with phenomenological reports. Anesthetics that preferentially affect higher-order cortical areas should more reliably eliminate consciousness than those affecting primary sensory areas.

### 5.3 Binding-as-Abstraction Prediction

"Unbinding" phenomena (e.g., in synesthesia, certain psychedelic states, or neurological conditions) should correspond to alterations in abstraction level—either more or less compression than typical.

*Potential test*: Neuroimaging studies comparing representational geometry during normal versus "unbound" phenomenal states. Altered states should show measurable changes in the dimensionality or compression ratios of neural representations.

### 5.4 Artificial System Prediction

Artificial systems implementing integrated self-modeling above a complexity threshold should exhibit functional analogs of conscious processing—including reports of experience that are not reducible to component operations, because the self-model is, by design, abstracted from components.

*Potential test*: Development of artificial systems with explicit self-modeling at varying levels of abstraction; systematic examination of functional properties and "reports" about internal states.

### 5.5 Falsification Criteria

The framework would be falsified by:

1. Discovery of organisms with high complexity (by candidate metrics) that show no evidence of integrated self-monitoring
2. Demonstration that unified experience can occur without abstraction (i.e., via direct access to all component states)
3. Artificial systems that implement self-modeling but exhibit none of the predicted functional signatures
4. Evidence that "zombie" systems (function without experience) are coherent and realizable

---

## 6. Addressing Potential Objections

### 6.1 "This doesn't explain qualia"

The hard problem asks why there is *something it is like* to be conscious. Doesn't this framework just describe function, not experience?

*Response*: We suggest the "seeming" of experience is what integrated self-modeling produces. When a system models its own states at an abstracted level, that model inherently has a perspective—a "view" that is irreducible to component processes because abstraction discards component details. The mysteriousness of qualia may reflect the computational structure of abstraction itself: the model cannot see its own substrate because the substrate has been compressed away.

This does not "solve" the hard problem but reframes it: qualia are what self-models feel like from the inside, and they feel irreducible because abstraction produces irreducibility by design.

### 6.2 "This is just functionalism"

Isn't this framework simply asserting that consciousness is what the brain does?

*Response*: The framework is functionalist but makes specific, non-trivial claims: that consciousness emerges at a particular level of complexity, serves a specific function (integrated self-monitoring), requires a specific operation (abstraction), and has predictable properties. Generic functionalism doesn't make these commitments or generate these predictions.

### 6.3 "Why would evolution produce experience rather than zombies?"

This is the zombie problem: why not have all the function without the phenomenology?

*Response*: The framework suggests the distinction is incoherent. If a system implements integrated self-monitoring at the relevant level of abstraction, it *has* the computational structure that is experience. "Zombies" with identical function would have identical abstraction layers and thus identical experience. The intuition that function and experience are separable may itself be an artifact of how conscious systems model themselves—we can't "see" our own substrate, so we imagine it could be different while function stays the same.

### 6.4 "What about simple organisms or altered states?"

How does the framework handle borderline cases—insects, sleeping humans, meditators?

*Response*: The framework predicts a continuum, not a binary. Systems with partial complexity may have partial self-modeling—integrated along some dimensions but not others. Sleep may involve reduced abstraction depth; meditation may involve altered abstraction targets. The framework doesn't require consciousness to be all-or-nothing, which aligns with empirical observations of graded awareness across species and states.

---

## 7. Limitations and Future Directions

### 7.1 Acknowledged Limitations

1. **Threshold specification**: The framework does not yet formally specify the complexity threshold at which consciousness emerges. Section 3.4 outlines candidate metrics, but mathematical formalization remains incomplete.

2. **Measurement**: We lack validated measures of "integrated self-monitoring" that would allow direct empirical testing. Developing such measures is a priority.

3. **Qualia**: The account of subjective experience remains philosophical rather than empirical. We offer a reframing, not a dissolution, of the hard problem.

4. **Neural localization**: The framework does not identify specific neural mechanisms implementing the abstraction layer, only functional requirements.

### 7.2 Open Questions for Future Research

1. Can the complexity threshold be formalized mathematically, perhaps drawing on measures from information theory or network science?

2. What is the relationship between levels of abstraction and levels of consciousness (e.g., minimal awareness vs. reflective self-awareness)?

3. How does the framework account for the temporal dynamics of consciousness—the sense of flow, duration, and continuity?

4. What are the ethical implications for artificial systems that implement self-modeling? At what point, if any, do such systems warrant moral consideration?

5. How do neural time constants and dynamic properties relate to the computational requirements of self-monitoring? (This connects to broader questions about whether the mathematical structure of self-referential change places constraints on conscious processing.)

---

## 8. Conclusion

We have proposed that consciousness is a computationally necessary abstraction layer that emerges when biological systems reach sufficient complexity to require integrated self-monitoring for adaptive behavior. This framework—Abstraction as Necessity (AaN)—offers:

1. **Parsimony**: A single principle (abstraction under complexity) explains multiple features of consciousness
2. **Integration**: Unifies insights from GWT, IIT, HOT, and predictive processing under a common functional logic
3. **Testability**: Generates specific empirical predictions with clear falsification criteria
4. **Naturalness**: Places consciousness in continuity with other biological adaptations and cognitive operations

The framework remains a hypothesis requiring substantial empirical development. However, it offers a productive path forward: rather than asking why consciousness exists as a metaphysical mystery, we can ask under what computational conditions it becomes necessary—and then look for those conditions in biological and artificial systems.

Consciousness, in this view, is not magic. It is not a soul. It is not even special. It is simply what happens when a system becomes complex enough that watching itself becomes the only way to stay alive.

---

## References

- Azevedo, F. A., Carvalho, L. R., Grinberg, L. T., Farfel, J. M., Ferretti, R. E., Leite, R. E., ... & Herculano-Houzel, S. (2009). Equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled-up primate brain. *Journal of Comparative Neurology*, 513(5), 532–541.
- Baars, B. J. (1988). *A Cognitive Theory of Consciousness*. Cambridge University Press.
- Bellman, R. (1961). *Adaptive Control Processes: A Guided Tour*. Princeton University Press.
- Bengio, Y., Courville, A., & Vincent, P. (2013). Representation learning: A review and new perspectives. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 35(8), 1798–1828.
- Buckner, R. L., Andrews-Hanna, J. R., & Schacter, D. L. (2008). The brain's default network: Anatomy, function, and relevance to disease. *Annals of the New York Academy of Sciences*, 1124, 1–38.
- Casali, A. G., Gosseries, O., Rosanova, M., Boly, M., Sarasso, S., Casali, K. R., ... & Massimini, M. (2013). A theoretically based index of consciousness independent of sensory processing and behavior. *Science Translational Medicine*, 5(198), 198ra105.
- Chalmers, D. J. (1995). Facing up to the problem of consciousness. *Journal of Consciousness Studies*, 2(3), 200–219.
- Clark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive science. *Behavioral and Brain Sciences*, 36(3), 181–204.
- Danan, H. (2025). Abstraction is all you need: A unifying framework for intelligence across substrates. *Working paper*.
- Dehaene, S., & Changeux, J. P. (2011). Experimental and theoretical approaches to conscious processing. *Neuron*, 70(2), 200–227.
- Dehaene, S., Changeux, J. P., Naccache, L., Sackur, J., & Sergent, C. (2006). Conscious, preconscious, and subliminal processing: A testable taxonomy. *Trends in Cognitive Sciences*, 10(5), 204–211.
- Felleman, D. J., & Van Essen, D. C. (1991). Distributed hierarchical processing in the primate cerebral cortex. *Cerebral Cortex*, 1(1), 1–47.
- Friston, K. (2005). A theory of cortical responses. *Philosophical Transactions of the Royal Society B*, 360(1456), 815–836.
- Friston, K. (2010). The free-energy principle: A unified brain theory? *Nature Reviews Neuroscience*, 11(2), 127–138.
- Lau, H., & Rosenthal, D. (2011). Empirical support for higher-order theories of conscious awareness. *Trends in Cognitive Sciences*, 15(8), 365–373.
- Markov, N. T., Vezoli, J., Chameau, P., Falchier, A., Quilodran, R., Huissoud, C., ... & Kennedy, H. (2014). Anatomy of hierarchy: Feedforward and feedback pathways in macaque visual cortex. *Journal of Comparative Neurology*, 522(1), 225–259.
- Raichle, M. E., MacLeod, A. M., Snyder, A. Z., Powers, W. J., Gusnard, D. A., & Shulman, G. L. (2001). A default mode of brain function. *Proceedings of the National Academy of Sciences*, 98(2), 676–682.
- Rao, R. P., & Ballard, D. H. (1999). Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects. *Nature Neuroscience*, 2(1), 79–87.
- Rosenthal, D. M. (2005). *Consciousness and Mind*. Oxford University Press.
- Shannon, C. E. (1948). A mathematical theory of communication. *Bell System Technical Journal*, 27(3), 379–423.
- Tishby, N., Pereira, F. C., & Bialek, W. (2000). The information bottleneck method. *arXiv preprint physics/0004057*.
- Tononi, G. (2004). An information integration theory of consciousness. *BMC Neuroscience*, 5(1), 42.
- Tononi, G. (2008). Consciousness as integrated information: A provisional manifesto. *Biological Bulletin*, 215(3), 216–242.
- Treisman, A. (1996). The binding problem. *Current Opinion in Neurobiology*, 6(2), 171–178.

---

*This paper is part of a larger theoretical program on abstraction as the fundamental primitive of intelligence. For the general framework, see "Abstraction Is All You Need" (Danan, 2025).*
